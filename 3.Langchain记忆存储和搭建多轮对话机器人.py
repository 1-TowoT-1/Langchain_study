from langchain_core.messages import AIMessage,HumanMessage,SystemMessage
from langchain.prompts import MessagesPlaceholder
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
import asyncio

# åˆå§‹åŒ–å¤§æ¨¡å‹
model = ChatOllama(
    model = "deepseek-r1:1.5b",
    base_url = "http://localhost:11434/",
)

# åˆå§‹åŒ–è¾“å‡ºè§£æå™¨
parser = StrOutputParser()

# åˆå§‹åŒ–promptæç¤ºè¯æ¨¡æ¿
prompt = ChatPromptTemplate([
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ˜¯ç”Ÿç‰©ä¿¡æ¯å­¦æ–¹é¢çš„ä¸“å®¶ï¼Œå¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚"),
    # MessagesPlaceholder åœ¨å¯¹è¯é“¾(conversation chain)ä¸­é¢„ç•™æ¶ˆæ¯çš„ä½ç½®ã€‚
    MessagesPlaceholder(variable_name="chat_history")
])

# åˆ›å»ºé“¾
basical_chain = prompt | model | parser

# åˆå§‹åŒ–å¯¹è¯å†å²
messages_list = []
print("* è¾“å…¥ exit/quit ç»“æŸå¯¹è¯ï¼")

async def chat_loop(messages_list):
    while True:
        user_query = input("ğŸ‘¤ ä½ ï¼š")
        if user_query.lower() in ['quit','exit']:
            break
        
        # æ·»åŠ å†å²å¯¹è¯ï¼Œæ·»åŠ çš„å†…å®¹æ˜¯HumanMessageå¯¹è±¡
        messages_list.append(HumanMessage(content=user_query))
        
        # è°ƒç”¨æ¨¡å‹æµå¼ç”Ÿæˆå“åº”
        ai_query_stream = basical_chain.astream({"chat_history": messages_list})  
        full_response = ""
        async for chunk in ai_query_stream:
            # flush æ˜¯å¼ºåˆ¶ç«‹å³åˆ·æ–°è¾“å‡ºç¼“å†²åŒºï¼Œç¡®ä¿å†…å®¹å®æ—¶æ˜¾ç¤ºåœ¨ç»ˆç«¯ï¼Œè€Œä¸æ˜¯ç­‰å¾…ç¼“å†²åŒºæ»¡æˆ–æ¢è¡Œæ—¶æ‰è¾“å‡ºã€‚
            print(chunk, end="", flush=True)
            full_response += chunk
        print()

        # æ·»åŠ AIå›å¤åˆ°å†å²å¯¹è¯ä¸­
        messages_list.append(AIMessage(content=full_response))

        # ä¿ç•™æœ€å100æ¡å¯¹è¯è®°å½•
        messages_list = messages_list[-100:]

# è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
asyncio.run(chat_loop(messages_list))